{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Does the loss go down?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini-container:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "###Â define utilities:\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def dual_opt(var_name_1, var_name_2, loss, optimizer):\n",
        "    \n",
        "    vars_1 = tf.get_collection(key = tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                     scope= var_name_1)\n",
        "    train_1 = optimizer.minimize(loss,var_list=vars_1)\n",
        "        \n",
        "    vars_2 = tf.get_collection(key = tf.GraphKeys.TRAINABLE_VARIABLES,\n",
        "                                     scope = var_name_2)\n",
        "    train_2 = optimizer.minimize(loss,var_list=vars_2)\n",
        "    \n",
        "    return tf.group(train_1, train_2)\n",
        "\n",
        "def action_states(env,agent,actions):\n",
        "\n",
        "    ss_ = np.concatenate((env.state_seq[env.iter-agent.horizon-1],env.state_seq[env.iter-1])).reshape((1,4))\n",
        "    S = np.repeat(ss_,agent.horizon,axis=0)\n",
        "            \n",
        "    return np.concatenate((actions,S),axis=1)\n",
        "\n",
        "### define environment:\n",
        "class square_env:\n",
        "    def __init__(self,duration,radius,dimension):\n",
        "        if 2*radius > dimension:\n",
        "            raise Warning(\"diameter can't exceed dimensions\")\n",
        "        self.R = radius # radius of agent\n",
        "        self.dimension = dimension # LxW of the square world\n",
        "        self.eps = radius/100\n",
        "        self.lower_limit, self.upper_limit = self.R+self.eps, self.dimension-self.R-self.eps\n",
        "        self.iter = 0 # current iteration\n",
        "        self.duration = duration # maximum duration of agent in environment\n",
        "        self.state_seq = np.zeros((self.duration,2))\n",
        "                \n",
        "    def random_initialisation(self):\n",
        "        # method for initialisation: \n",
        "            \n",
        "        self.state_seq[self.iter][0] = np.random.uniform(self.lower_limit, self.upper_limit)\n",
        "        self.state_seq[self.iter][1] = np.random.uniform(self.lower_limit, self.upper_limit)\n",
        "        \n",
        "        self.iter = 1\n",
        "        \n",
        "    def boundary_conditions(self):\n",
        "                \n",
        "        #boundary conditions:\n",
        "        cond_X = (self.state_seq[self.iter-1][0] >= self.lower_limit)*(self.state_seq[self.iter-1][0] <= self.upper_limit)\n",
        "        cond_Y = (self.state_seq[self.iter-1][1] >= self.lower_limit)*(self.state_seq[self.iter-1][1] <= self.upper_limit)\n",
        "\n",
        "        return cond_X*cond_Y\n",
        "            \n",
        "    def step(self, action):\n",
        "                \n",
        "        self.state_seq[self.iter] = self.state_seq[self.iter-1] + action\n",
        "        \n",
        "        #return to previous state if boundary conditions are not satisfied:\n",
        "        if self.boundary_conditions() == 0:\n",
        "            self.state_seq[self.iter] -= action\n",
        "            \n",
        "        self.iter += 1\n",
        "            \n",
        "        if self.iter > self.duration:\n",
        "            raise Exception(\"Game over!\")            \n",
        "            \n",
        "            \n",
        "    def env_response(self,actions,horizon):\n",
        "        # update the environment\n",
        "        \n",
        "        for i in range(1,horizon):\n",
        "            self.step(actions[i])\n",
        "        \n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Return to the initial conditions. \n",
        "        \"\"\"\n",
        "        self.state_seq = np.zeros((self.duration,2))\n",
        "        self.iter = 0\n",
        "        \n",
        "        \n",
        "### define agent:\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class agent_cognition:\n",
        "    \n",
        "    \"\"\"\n",
        "        An agent that reasons using a measure of empowerment. \n",
        "        Here we assume that env refers to an initialised environment class. \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,planning_horizon,sess,seed, bound):\n",
        "        self.sess = sess\n",
        "        self.seed = seed\n",
        "        self.horizon = planning_horizon        \n",
        "        self.bound = bound\n",
        "        \n",
        "        self.current_state = tf.placeholder(tf.float32, [None, 2])\n",
        "        self.source_action = tf.placeholder(tf.float32, [None, 2])\n",
        "        # define a placeholder for beta values in the squared loss:\n",
        "        self.beta = tf.placeholder(tf.float32, [None, 1])\n",
        "        \n",
        "        ## define a placeholder for the dropout value:\n",
        "        self.prob = tf.placeholder_with_default(1.0, shape=(),name='prob')\n",
        "        \n",
        "        ## define a placeholder for the learning rate:\n",
        "        self.lr = tf.placeholder(tf.float32, shape = [],name='lr')\n",
        "        \n",
        "        ## define empowerment critic:\n",
        "        self.emp = self.empowerment_critic()\n",
        "                \n",
        "        ## define source:\n",
        "        self.source_input_n = tf.placeholder(tf.float32, [None, 4])\n",
        "        self.src_mu, self.src_log_sigma = self.source_dist_n()\n",
        "        self.src_dist = tfp.distributions.MultivariateNormalDiag(self.src_mu, \\\n",
        "                                                             tf.exp(self.src_log_sigma))\n",
        "                            \n",
        "        self.log_src = self.src_dist.log_prob(self.source_action)\n",
        "        \n",
        "        \n",
        "        ## define decoder parameters and log probability:\n",
        "        self.decoder_input_n = tf.placeholder(tf.float32, [None, 6])\n",
        "        self.decoder_mu, self.decoder_log_sigma = self.decoder_dist_n()\n",
        "        \n",
        "        self.decoder_dist = tfp.distributions.MultivariateNormalDiag(self.decoder_mu, \\\n",
        "                                                             tf.exp(self.decoder_log_sigma))\n",
        "        \n",
        "        self.log_decoder = self.decoder_dist.log_prob(self.source_action)\n",
        "        \n",
        "        ## define losses:\n",
        "        self.decoder_loss = tf.reduce_mean(-1.0*self.decoder_dist.log_prob(self.source_action))\n",
        "        \n",
        "        self.squared_loss = tf.reduce_mean(tf.square(self.beta*self.log_decoder-self.emp-self.log_src))\n",
        "        \n",
        "        ### define the optimisers:\n",
        "        self.fast_optimizer = tf.train.AdagradOptimizer(self.lr,name='ada_1')\n",
        "        self.slow_optimizer = tf.train.AdagradOptimizer(self.lr,name='ada_2')\n",
        "        \n",
        "        self.train_decoder = self.fast_optimizer.minimize(self.decoder_loss)\n",
        "        \n",
        "        ### define a dual optimizatio method for critic and source:\n",
        "        self.train_critic_and_source = dual_opt(\"critic\", \"source\", self.squared_loss,\\\n",
        "                                                self.slow_optimizer)\n",
        "        \n",
        "    \n",
        "        self.init_g = tf.global_variables_initializer() \n",
        "    \n",
        "    def init_weights(self,shape,var_name):\n",
        "        \"\"\"\n",
        "            Xavier initialisation of neural networks\n",
        "        \"\"\"\n",
        "        initializer = tf.contrib.layers.xavier_initializer()\n",
        "        return tf.Variable(initializer(shape),name = var_name)\n",
        "        \n",
        "    def two_layer_net(self, X, w_h, w_h2, w_o,bias_1, bias_2):\n",
        "        \"\"\"\n",
        "            A generic method for creating two-layer networks\n",
        "            \n",
        "            input: weights\n",
        "            output: neural network\n",
        "        \"\"\"\n",
        "        \n",
        "        h = tf.nn.elu(tf.add(tf.matmul(X, w_h),bias_1))\n",
        "        drop_1 = tf.nn.dropout(h, self.prob)\n",
        "        \n",
        "        h2 = tf.nn.elu(tf.add(tf.matmul(drop_1, w_h2),bias_2))\n",
        "        drop_2 = tf.nn.dropout(h2, self.prob)\n",
        "        \n",
        "        return tf.matmul(drop_2, w_o)\n",
        "    \n",
        "    def empowerment_critic(self):\n",
        "        \"\"\"\n",
        "        This function provides a cheap approximation to empowerment\n",
        "        upon convergence of the training algorithm. Given that the \n",
        "        mutual information is non-negative this function must only\n",
        "        give non-negative output. \n",
        "        \n",
        "        input: state\n",
        "        output: empowerment estimate\n",
        "        \"\"\"\n",
        "        \n",
        "        #with tf.variable_scope(\"critic\",reuse=tf.AUTO_REUSE):\n",
        "        with tf.variable_scope(\"critic\"):\n",
        "            \n",
        "            tf.set_random_seed(self.seed)\n",
        "    \n",
        "            w_h = self.init_weights([2,500],\"w_h\")\n",
        "            w_h2 = self.init_weights([500,300],\"w_h2\")\n",
        "            w_o = self.init_weights([300,1],\"w_o\")\n",
        "            \n",
        "            ### bias terms:\n",
        "            bias_1 = self.init_weights([500],\"bias_1\")\n",
        "            bias_2 = self.init_weights([300],\"bias_2\")\n",
        "            bias_3 = self.init_weights([1],\"bias_3\")\n",
        "                \n",
        "            h = tf.nn.elu(tf.add(tf.matmul(self.current_state, w_h),bias_1))\n",
        "            h2 = tf.nn.elu(tf.add(tf.matmul(h, w_h2),bias_2))\n",
        "        \n",
        "        return tf.nn.elu(tf.add(tf.matmul(h2, w_o),bias_3))\n",
        "        \n",
        "        \n",
        "    def source_dist_n(self):\n",
        "        \n",
        "        \"\"\"\n",
        "            This is the per-action source distribution, also known as the \n",
        "            exploration distribution. \n",
        "        \"\"\"\n",
        "        \n",
        "        #with tf.variable_scope(\"source\",reuse=tf.AUTO_REUSE):\n",
        "        with tf.variable_scope(\"source\"):\n",
        "                               \n",
        "            tf.set_random_seed(self.seed)\n",
        "            \n",
        "            W_h = self.init_weights([4,300],\"W_h\")\n",
        "            W_h2 = self.init_weights([300,100],\"W_h2\")\n",
        "            W_o = self.init_weights([100,10],\"W_o\")\n",
        "            \n",
        "            # define bias terms:\n",
        "            bias_1 = self.init_weights([300],\"bias_1\")\n",
        "            bias_2 = self.init_weights([100],\"bias_2\")\n",
        "            \n",
        "            ## two-layer network:\n",
        "            h = tf.nn.elu(tf.add(tf.matmul(self.source_input_n, W_h),bias_1))\n",
        "            drop_1 = tf.nn.dropout(h, self.prob)\n",
        "        \n",
        "            h2 = tf.nn.elu(tf.add(tf.matmul(drop_1, W_h2),bias_2))\n",
        "            drop_2 = tf.nn.dropout(h2, self.prob)\n",
        "            \n",
        "            Tau = tf.matmul(drop_2, W_o)\n",
        "                                    \n",
        "            W_mu = self.init_weights([10,2],\"W_mu\")\n",
        "            W_sigma = self.init_weights([10,2],\"W_sigma\")\n",
        "            \n",
        "            mu = tf.matmul(Tau,W_mu)\n",
        "            log_sigma = tf.multiply(tf.nn.tanh(tf.matmul(Tau,W_sigma)),self.bound)\n",
        "            \n",
        "        \n",
        "        return mu, log_sigma\n",
        "    \n",
        "    \n",
        "    def sampler(self,mu,log_sigma):\n",
        "                        \n",
        "        return np.random.normal(mu,np.exp(log_sigma))   \n",
        "    \n",
        "    def random_actions(self):\n",
        "        \"\"\"\n",
        "            This baseline is used as a drop in replacement for the source at the\n",
        "            early stages of learning and to check that the source isn't completely useless. \n",
        "        \"\"\"\n",
        "        \n",
        "        return np.random.normal(0,self.bound,size = (self.horizon,2))\n",
        "        \n",
        "    \n",
        "    def source_actions(self,state):\n",
        "        \n",
        "        actions = np.zeros((self.horizon,2))\n",
        "        \n",
        "        ### add a zero action to the state:\n",
        "        AS_0 = np.concatenate((np.zeros(2),state))\n",
        "        \n",
        "        mu, log_sigma = self.sess.run([self.src_mu,self.src_log_sigma], feed_dict={ self.source_input_n: AS_0.reshape((1,4))})\n",
        "                                                \n",
        "        for i in range(1,self.horizon):\n",
        "                        \n",
        "            AS_n = np.concatenate((actions[i-1],state))\n",
        "            \n",
        "            mu, log_sigma = self.sess.run([self.src_mu,self.src_log_sigma], feed_dict={ self.source_input_n: AS_n.reshape((1,4))})\n",
        "                        \n",
        "            actions[i] = self.sampler(mu, log_sigma)\n",
        "                    \n",
        "        return actions\n",
        "        \n",
        "    def decoder_dist_n(self): \n",
        "        \n",
        "        \"\"\"\n",
        "            This is the per-action decoder, also known as the \n",
        "            planning distribution. \n",
        "        \"\"\"\n",
        "        \n",
        "        #with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
        "        with tf.variable_scope(\"decoder\"):\n",
        "            \n",
        "            tf.set_random_seed(self.seed)\n",
        "                        \n",
        "            W_h = self.init_weights([6,300],\"W_h\")\n",
        "            W_h2 = self.init_weights([300,100],\"W_h2\")\n",
        "            W_o = self.init_weights([100,10],\"W_o\")\n",
        "            \n",
        "            # define bias terms:\n",
        "            bias_1 = self.init_weights([300],\"bias_1\")\n",
        "            bias_2 = self.init_weights([100],\"bias_2\")\n",
        "            \n",
        "            ## two-layer network:\n",
        "            h = tf.nn.elu(tf.add(tf.matmul(self.decoder_input_n, W_h),bias_1))\n",
        "            h2 = tf.nn.elu(tf.add(tf.matmul(h, W_h2),bias_2))\n",
        "            \n",
        "            Tau = tf.matmul(h2, W_o)\n",
        "                        \n",
        "            W_mu = self.init_weights([10,2],\"W_mu\")\n",
        "            W_sigma = self.init_weights([10,2],\"W_sigma\")\n",
        "            \n",
        "            mu = tf.matmul(Tau,W_mu)\n",
        "            log_sigma = tf.multiply(tf.nn.tanh(tf.matmul(Tau,W_sigma)),self.bound)\n",
        "                    \n",
        "            \n",
        "        return mu, log_sigma\n",
        "    \n",
        "    def decoder_actions(self,ss_):\n",
        "        \n",
        "        actions = np.zeros((self.horizon,2))\n",
        "        \n",
        "        ### add a zero action to the state:\n",
        "        SS_0 = np.concatenate((np.zeros(2),ss_))\n",
        "        \n",
        "        mu, log_sigma = self.sess.run([self.decoder_mu,self.decoder_log_sigma], feed_dict={ self.decoder_input_n: SS_0.reshape((1,6))})\n",
        "                                                \n",
        "        for i in range(1,self.horizon):\n",
        "                        \n",
        "            SS_n = np.concatenate((actions[i-1],ss_))\n",
        "    \n",
        "            mu, log_sigma = self.sess.run([self.decoder_mu,self.decoder_log_sigma], feed_dict={ self.decoder_input_n: SS_n.reshape((1,6))})\n",
        "                                \n",
        "            actions[i] = self.sampler(mu, log_sigma)\n",
        "                    \n",
        "        return actions\n",
        "    \n",
        "    def mean_decoder_actions(self,ss_):\n",
        "        \n",
        "        actions, sigmas = np.zeros((self.horizon,2)), np.zeros((self.horizon,2))\n",
        "        \n",
        "        ### add a zero action to the state:\n",
        "        SS_0 = np.concatenate((np.zeros(2),ss_))\n",
        "        \n",
        "        mu, log_sigma = self.sess.run([self.decoder_mu,self.decoder_log_sigma], feed_dict={ self.decoder_input_n: SS_0.reshape((1,6))})\n",
        "                                                \n",
        "        for i in range(1,self.horizon):\n",
        "                        \n",
        "            SS_n = np.concatenate((actions[i-1],ss_))\n",
        "    \n",
        "            mu, log_sigma = self.sess.run([self.decoder_mu,self.decoder_log_sigma], feed_dict={ self.decoder_input_n: SS_n.reshape((1,6))})\n",
        "                                \n",
        "            actions[i], sigmas[i] = mu, np.exp(log_sigma)\n",
        "                    \n",
        "        return actions, sigmas\n",
        "                    \n",
        "        return actions, sigmas"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Define experiment:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "## set random seed:\n",
        "tf.set_random_seed(42)\n",
        "\n",
        "prob = 0.8\n",
        "\n",
        "def training(seed, batch_size, lr,iters, horizon,bound):\n",
        "    # define epoch counter:\n",
        "    count = 0\n",
        "    \n",
        "    # define environment:    \n",
        "    env = square_env(duration=horizon,radius=0.5,dimension=2*horizon*0.5)\n",
        "        \n",
        "    with tf.Session() as sess:\n",
        "                \n",
        "        A = agent_cognition(horizon,sess,seed,bound)   \n",
        "        \n",
        "        ### define beta schedule:\n",
        "        betas = 1./np.array([min(0.001 + i/iters,1) for i in range(iters)])\n",
        "                \n",
        "        ## define inverse probability:\n",
        "        inverse_prob = betas\n",
        "        \n",
        "        ### initialise the variables:\n",
        "        sess.run(A.init_g)\n",
        "        \n",
        "        squared_losses, decoder_losses = np.zeros(iters), np.zeros(iters)\n",
        "        \n",
        "        for count in range(iters):\n",
        "            \n",
        "            ## reset the environment:\n",
        "            env.reset()\n",
        "            env.random_initialisation()\n",
        "            \n",
        "            mini_batch = np.zeros((batch_size*horizon,6))\n",
        "            \n",
        "            ## define mean and variance of environment:\n",
        "            mu = env.dimension/2.0 - 0.5 ## mean of U(R,dimension-R)\n",
        "            sigma = ((2*mu)**2)/12 ## variance of U(R,dimension-R)\n",
        "            \n",
        "            if count % 100 == 0:\n",
        "                print(count)\n",
        "            \n",
        "            ### train our agent on a minibatch of recent experience:\n",
        "            for i in range(batch_size):\n",
        "                \n",
        "                env.iter = 0\n",
        "                                            \n",
        "                if np.random.rand() > 1/inverse_prob[count]:\n",
        "                    actions = A.random_actions()\n",
        "                else:\n",
        "                    normalised_state = (env.state_seq[env.iter]-mu)/sigma\n",
        "                    \n",
        "                    actions = A.source_actions(normalised_state)\n",
        "                    \n",
        "                env.iter += 1\n",
        "                        \n",
        "                ## get responses from the environment:\n",
        "                env.env_response(actions,A.horizon)\n",
        "                                    \n",
        "                ## group actions, initial state, and final state:                        \n",
        "                axx_ = action_states(env,A,actions)\n",
        "                \n",
        "                mini_batch[horizon*i:horizon*(i+1)] = axx_\n",
        "            \n",
        "            ## normalise the state representations:\n",
        "            mini_batch[:,2:6] = (mini_batch[:,2:6] - mu)/sigma\n",
        "                \n",
        "            train_feed_1 = {A.decoder_input_n : mini_batch,A.source_action : mini_batch[:,0:2],\\\n",
        "                            A.prob : prob,A.lr:lr}\n",
        "            \n",
        "            sess.run(A.train_decoder,feed_dict = train_feed_1)\n",
        "                \n",
        "            # train source and critic:\n",
        "            train_feed_2 = {A.beta: betas[count].reshape((1,1)), A.current_state: mini_batch[:,2:4],\\\n",
        "                            A.decoder_input_n : mini_batch, A.source_input_n : mini_batch[:,0:4], \\\n",
        "                            A.source_action : mini_batch[:,0:2],\n",
        "                            A.prob : prob,A.lr:lr}\n",
        "            \n",
        "            sess.run(A.train_critic_and_source,feed_dict = train_feed_2)\n",
        "                \n",
        "            squared_losses[count] = sess.run(A.squared_loss,feed_dict = train_feed_2)\n",
        "            decoder_losses[count] = sess.run(A.decoder_loss,feed_dict = train_feed_1)        \n",
        "            \n",
        "        return decoder_losses, squared_losses"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Run first experiment:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 3\n",
        "seed = [42,43]\n",
        "bound = 1.0\n",
        "iters = 10000 \n",
        "batch_size = 50\n",
        "num_expts = 2\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "decoder_losses = np.zeros((num_expts,iters))\n",
        "squared_losses = np.zeros((num_expts,iters))\n",
        "\n",
        "for i in range(num_expts):\n",
        "    decoder_losses[i], squared_losses[i] = training(seed[i], batch_size,lr, iters, horizon,bound)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-73ce006a0f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_expts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdecoder_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-0b78d81f5e1c>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(seed, batch_size, lr, iters, horizon, bound)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_critic_and_source\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feed_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0msquared_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquared_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feed_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mdecoder_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feed_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Visualizing the relationship:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "f, axarr = plt.subplots(4, sharex=True,figsize=(20,10))\n",
        "#plt.figure(figsize=(10,10))\n",
        "axarr[0].set_title(\"first decoder loss\")\n",
        "axarr[0].plot(decoder_losses[0],'steelblue')\n",
        "\n",
        "axarr[1].set_title(\"second decoder loss\")\n",
        "axarr[1].plot(decoder_losses[1],'steelblue')\n",
        "\n",
        "axarr[2].set_title(\"first squared loss\")\n",
        "axarr[2].plot(squared_losses[0],'crimson')\n",
        "\n",
        "axarr[3].set_title(\"second squared loss\")\n",
        "axarr[3].plot(squared_losses[1],'crimson')\n",
        "\nplt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â 4. Analysis of the loss curves: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that the loss goes to zero very quickly and then stabilizes. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_losses[0][10],decoder_losses[0][100],decoder_losses[0][1000],  decoder_losses[0][-1]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "squared_losses[0][10],squared_losses[0][100],squared_losses[0][1000], squared_losses[0][-1]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the squared loss is still decreasing and that the decoder loss has basically decreased a lot faster than the squared loss. A reasonable next experiment would be to double\n",
        "the number of iterations and see whether the squared loss approaches 0. I reckon that the reason why the decoder loss goes down more quickly is that it involves a maximum likelihood involving only one non-linear function."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â 5. Second experiment: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 3\n",
        "seed = [42,43]\n",
        "bound = 1.0\n",
        "iters = 20000 \n",
        "batch_size = 50\n",
        "num_expts = 2\n",
        "\n",
        "lr_1, lr_2 = 0.01, 0.01\n",
        "\n",
        "decoder_losses = np.zeros((num_expts,iters))\n",
        "squared_losses = np.zeros((num_expts,iters))\n",
        "\n",
        "for i in range(num_expts):\n",
        "    decoder_losses[i], squared_losses[i] = training(seed[i], batch_size,lr_1, lr_2, iters, horizon,bound)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Visualize the loss curves: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "f, axarr = plt.subplots(4, sharex=True,figsize=(20,10))\n",
        "#plt.figure(figsize=(10,10))\n",
        "axarr[0].set_title(\"first decoder loss\")\n",
        "axarr[0].plot(decoder_losses[0],'steelblue')\n",
        "\n",
        "axarr[1].set_title(\"second decoder loss\")\n",
        "axarr[1].plot(decoder_losses[1],'steelblue')\n",
        "\n",
        "axarr[2].set_title(\"first squared loss\")\n",
        "axarr[2].plot(squared_losses[0],'crimson')\n",
        "\n",
        "axarr[3].set_title(\"second squared loss\")\n",
        "axarr[3].plot(squared_losses[1],'crimson')\n",
        "\nplt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Â 7. Analysis of the loss curves: "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "squared_losses[0][10],squared_losses[0][100],squared_losses[0][1000],squared_losses[0][10000], squared_losses[0][-1]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_losses[1][10],decoder_losses[1][100],decoder_losses[1][1000],decoder_losses[0][10000],  decoder_losses[1][-1]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}